{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smvDyGWp7vko"
      },
      "source": [
        "# Automatic Data Downloads\n",
        "Satellite images and outputs from global earth systems models can be very large files. If we're dealing with time series, large spatial areas, or multivariate model outputs, we can quickly be moving into data volumes that exceed the memory and storage capacity of personal computers. To access these types of global data, we are interfacing with online databases. Today's lesson is intended to give you the tools to programmatically access online databases. These tools will enable you to use your personal computer to convert these large datasets into analysis-ready data for your research project. Specifically, today we'll learn to:\n",
        "\n",
        "1. Interpret directory structure of ftp and http addresses.\n",
        "2. Create a project directory on your local machine.\n",
        "3. Configure a .gitignore file to ignore raw data.\n",
        "4. Use the command line to download files from the internet.\n",
        "\n",
        "If there's time, we'll break into groups based on research interest and start utilizing APIs to search datasets on public geospatial data repositories that match the location and time period of your study area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "unL86Vf57vkr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR0ZcsJ-7vks"
      },
      "source": [
        "## G is for *Generalizable*\n",
        "When we're making measurements of an earth system process, we often care deeply about how well uur experimental results apply to other times/places. Since it is often too expensive or two difficult to collect in-situ samples of our earth systems process at all the times and locations that matter, environmental data science allows us to use statistical models to leverage globally available observations to improve the generalizability of our system. These models will generalize our inferences about our earth systems process in one of three ways:\n",
        "\n",
        "1. *Prediction*: can our model allow us to generalize our observations to out-of-sample times and locations? For example: will my model linking air temperature to green-up time from my experimental forest accurately apply to a forest 200 miles away?\n",
        "2. *Interpolation*: can our model allow us to \"fill in the gaps\" in our spatial/temporal sampling schele? For example: do my measurements of precipitation for my two precipitation gage locations accurately represent the total precipitation that fell in my watershed?\n",
        "3. *Diagnosis*: can our model help us to interpret what processes are either drivers of or covariates with our earth systems process, allowing us to improve our physical understanding of trends and variability in that system: for example: is air temperature or precipitation a more important driver of current cropping system productivity, and how might this impact cropping system function under climate change?\n",
        "\n",
        "### These global observations are often publically available to researchers on online geodatabases.\n",
        "For example:\n",
        " - NASA: https://earthdata.nasa.gov/\n",
        " - USGS: https://earthexplorer.usgs.gov/\n",
        " - NOAA: https://psl.noaa.gov/data/gridded/\n",
        " - Google Earth Engine: https://developers.google.com/earth-engine/datasets\n",
        " - NY State: https://cugir.library.cornell.edu/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIAOL8e67vkt"
      },
      "source": [
        "## R is for *Reproducible*\n",
        "Since the raw data for our generalizable analysis is globally available, programmatically accessing our data gives us an important added benefit: we can design our version controlled, collaborative project repositories so they directly interface with these public geodatabases. That way, anyone who wants to can access the raw data required to reproduce our analytic workflow.\n",
        "\n",
        "A reminder on why reproducible science is so important:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "km7UBlVx7vkt",
        "outputId": "b58b9868-55fa-4dd7-dbb6-99ccaa48f18a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/display.py:701: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"930\" height=\"523\" src=\"https://www.youtube.com/embed/NGFO0kdbZmk\", frameborder=\"0\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "HTML('<iframe width=\"930\" height=\"523\" src=\"https://www.youtube.com/embed/NGFO0kdbZmk\", frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOgH4VCO7vku"
      },
      "source": [
        "### Project Repository\n",
        "Your project repository is where you store all of the elements of your data science workflow. At it's core, it should have folders for raw data, processed data, code, outputs, and images. A good project repository is.\n",
        "\n",
        "1. Human readable: use directory names that are easy to understand, includes a highly detailed README file that explains what's in each folder, how to sequence inputs and outputs to code files, and how to cite the repository.\n",
        "2. Machine readable - avoid funky characters OR SPACES.\n",
        "3. Supportive of sorting - If you have a list of input files, it’s nice to be able to sort them to quickly see what’s there and find what you need.\n",
        "\n",
        "You should also take extra steps to preserve raw data so it’s not modified. More on this later.\n",
        "\n",
        "We're going to create a new repository for your class project. The os package (os stands for **O**perating **S**ystem) allows you to manipulate files on your computer. Ask it what it does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Idxjw8Ut7vkv"
      },
      "outputs": [],
      "source": [
        "?os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-m-ftvT7vkw"
      },
      "outputs": [],
      "source": [
        "#For example, this command is the equivalent of ls in terminal:\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qp81aQ4F7vkw"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL IF YOU ARE ON A PC!\n",
        "\n",
        "#this command is the equivalent of:\n",
        "# mkdir C:/EnvDatSci/project\n",
        "os.mkdir('C:\\\\EnvDatSci\\\\project')\n",
        "\n",
        "#this command is the equivalent of:\n",
        "# cd C:/EnvDatSci/project\n",
        "os.chdir('C:\\\\EnvDatSci\\\\project')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL IF YOU ARE ON GOOGLE COLAB!\n",
        "\n",
        "#this command is the equivalend of:\n",
        "# mkdir content/drive/MyDrive/EnvDatSci/project\n",
        "\n",
        "# this command is the equivalent of:\n",
        "# cd  content/drive/MyDrive/EnvDatSci/project\n"
      ],
      "metadata": {
        "id": "5r1q9Cln8lMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIDD78XS7vkx"
      },
      "source": [
        "### TASK 1: enter a command in the below cell to check and make sure you're in your project directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zszpDQEv7vky"
      },
      "outputs": [],
      "source": [
        "#Task 1:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OscYwl_7vkz"
      },
      "source": [
        "### TASK 2: populate your project directory with appropriate files\n",
        "Read Chapter 4.1 of the textbook: https://www.earthdatascience.org/courses/earth-analytics/document-your-science/file-organization-101/\n",
        "\n",
        "Using os commands, populate your project directory with subfolders.\n",
        "\n",
        "Print your directory to the screen (hint: see Task 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKAKIibY7vkz"
      },
      "outputs": [],
      "source": [
        "#Task 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCscDoxO7vk0"
      },
      "source": [
        "### TASK 3: change the current working directory to your the folder where you intend to store raw data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npKo69Rg7vk0"
      },
      "outputs": [],
      "source": [
        "#Task 3:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1uADeoz7vk0"
      },
      "source": [
        "## Decoding the file structure of online geodatabases\n",
        "Just like we can use code to find and access files on our local machine, we can use code to find and access files on public geodatabases. Since these geodatabases are version controlled, providing code that links to the online files helps prevent us from making redundant copies of data on the internet. Programatically accessing public geodatabases requires that we understand how the database itself has been organized.\n",
        "\n",
        " - Click on the following link to the National Oceanic and Atmospheric Association databse website: https://psl.noaa.gov/data/gridded/\n",
        "\n",
        " - Navigate to the \"NCEP/NCAR Reanalysis dataset\"\n",
        " - Of the seven sections they've divided data into, click on \"Surface\"\n",
        " - Under \"Air Temperature: Daily\", click \"See list\"\n",
        " - Under \"Surface\", click \"See list\"\n",
        "\n",
        "### TASK 4: Right click on the first link in the list, and select \"copy link\". Paste that link address below:\n",
        "https://downloads.psl.noaa.gov/Datasets/ncep.reanalysis.dailyavgs/surface/air.sig995.1948.nc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfmUoPsN7vk1"
      },
      "source": [
        "##### Task 4: double click on this markdown cell to add text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53K2xtki7vk1"
      },
      "source": [
        "### Tasking your computer to download files\n",
        "Our goal is to write a script that can download files, extract a relevant subset of information from the files, and then delete the files. The first part of this task to to learn the filenames that we want to download.\n",
        "\n",
        "In the link above, we can break the filepath down into substrings, using basic text commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waX9wXHE7vk2"
      },
      "outputs": [],
      "source": [
        "http_dir = \"https://downloads.psl.noaa.gov/Datasets/\"\n",
        "dataset = \"ncep.reanalysis.dailyavgs\"\n",
        "lev_type = \"surface\"\n",
        "variable = \"air.sig995.\"\n",
        "time = \"2010\"\n",
        "file_type = \".nc\"\n",
        "filepaths= http_dir + dataset + \"/\" + lev_type + \"/\" + variable + time + file_type\n",
        "print(filepaths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idVIpBXK7vk3"
      },
      "source": [
        "What happens if you click on that link? You can also have python download the file for you using the <urllib.request.urlretrieve> function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiW40GTk7vk3"
      },
      "outputs": [],
      "source": [
        "#what does this function do and how do we use it?\n",
        "?urllib.request.urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q3Ptnlo7vk4"
      },
      "outputs": [],
      "source": [
        "url = filepaths\n",
        "filename = variable + time + file_type\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "print(url, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO0bTLO17vk4"
      },
      "outputs": [],
      "source": [
        "#what happens?\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS2A7-wF7vk4"
      },
      "source": [
        "We can infer patterns from the database itself and generate the names of multiple files. For example, if we need five years of daily air temperature data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh71Pddj7vk4"
      },
      "outputs": [],
      "source": [
        "time =pd.Series(list(range(1965,1970)))\n",
        "time = time.apply(str)\n",
        "filepaths= http_dir + dataset + \"/\" + lev_type + \"/\" + variable + time + file_type\n",
        "print(filepaths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNi70_eu7vk5"
      },
      "source": [
        "### TASK 5: Write a \"for\" loop that downloads all five years worth of air temperature data into you working directory. Print the contents of your directory to the screen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fo-oaCa7vk5"
      },
      "outputs": [],
      "source": [
        "#Task 5\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}