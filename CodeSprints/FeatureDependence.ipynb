{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LizCarter492/EnvDatSci/blob/main/CodeSprints/FeatureDependence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204733d1",
      "metadata": {
        "id": "204733d1"
      },
      "source": [
        "\n",
        "## Feature Dependence and Regression Analysis\n",
        "In this example, we will be using a spatial dataset of county-level election and demographic statistics for the United States. We'll explore different methods to diagnose and account for multicollinearity in our data in regression analysis. Specifically, we'll calculate variance inflation factor (VIF), and compare parameter estimates and model fit in a multivariate regression predicting 2016 county voting preferences using an OLS model, a ridge regression, a lasso regression, and an elastic net regression.\n",
        "\n",
        "Objectives:\n",
        "* **Calculate a variance inflation factor to diagnose multicollinearity.**\n",
        "* **Interpret model summary statistics.**\n",
        "* **Describe how multicollinearity impacts stability in parameter esimates.**\n",
        "* **Explain the variance/bias tradeoff and describe how to use it to improve models**\n",
        "* **Draw a conclusion based on contrasting models.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install geopandas\n",
        "!pip install libpysal"
      ],
      "metadata": {
        "id": "nuph--LrdYaJ"
      },
      "id": "nuph--LrdYaJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf668f40",
      "metadata": {
        "id": "bf668f40"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import absolute\n",
        "from libpysal.weights.contiguity import Queen\n",
        "import libpysal\n",
        "from statsmodels.api import OLS\n",
        "sns.set_style('white')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daec4204",
      "metadata": {
        "id": "daec4204"
      },
      "source": [
        "First, we're going to load the 'Elections' dataset from the libpysal library, which is a very easy to use API that accesses the Geodata Center at the University of Chicago.\n",
        "\n",
        "* More on spatial data science resources from UC: https://spatial.uchicago.edu/\n",
        "* A list of datasets available through lipysal: https://geodacenter.github.io/data-and-lab//"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de38c04",
      "metadata": {
        "id": "9de38c04"
      },
      "outputs": [],
      "source": [
        "from libpysal.examples import load_example\n",
        "elections = load_example('Elections')\n",
        "#note the folder where your data now lives:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7fe6d5",
      "metadata": {
        "id": "8c7fe6d5"
      },
      "outputs": [],
      "source": [
        "#First, let's see what files are available in the 'Elections' data example\n",
        "elections.get_file_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d92c29a3",
      "metadata": {
        "id": "d92c29a3"
      },
      "source": [
        "When you are out in the world doing research, you often will not find a ready-made function to download your data. That's okay! You know how to get this dataset without using pysal! Do a quick internal review of online data formats and automatic data downloads.\n",
        "\n",
        "### TASK 1: Use urllib functions to download this file directly from the internet a folder (not in your git repository). Extract the zipped file you've downloaded into a subfolder called elections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1318b96b",
      "metadata": {
        "id": "1318b96b"
      },
      "outputs": [],
      "source": [
        "# Task 1 code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea133475",
      "metadata": {
        "id": "ea133475"
      },
      "source": [
        "### TASK 2: Use geopandas to read in this shapefile. Call your geopandas.DataFrame \"votes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81455057",
      "metadata": {
        "id": "81455057"
      },
      "outputs": [],
      "source": [
        "# TASK 2: Use geopandas to read in this shapefile. Call your geopandas.DataFrame \"votes\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ae67e1e",
      "metadata": {
        "id": "8ae67e1e"
      },
      "outputs": [],
      "source": [
        "#Let's view the shapefile to get a general idea of the geometry we're looking at:\n",
        "%matplotlib inline\n",
        "votes.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7495565e",
      "metadata": {
        "id": "7495565e"
      },
      "outputs": [],
      "source": [
        "#View the first few lines of the dataset\n",
        "votes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "813ad89f",
      "metadata": {
        "id": "813ad89f"
      },
      "outputs": [],
      "source": [
        "#Since there are too many columns for us to view on a signle page using \"head\", we can just print out the column names so we have them all listed for reference\n",
        "for col in votes.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa277c42",
      "metadata": {
        "id": "aa277c42"
      },
      "source": [
        "#### You can use pandas summary statistics to get an idea of how county-level data varies across the United States.\n",
        "### TASK 3: For example, how did the county mean percent Democratic vote change between 2012 (pct_dem_12) and 2016 (pct_dem_16)?\n",
        "\n",
        "Look here for more info on pandas dataframes:\n",
        "\n",
        "https://www.earthdatascience.org/courses/intro-to-earth-data-science/scientific-data-structures-python/pandas-dataframes/run-calculations-summary-statistics-pandas-dataframes/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "818f70e5",
      "metadata": {
        "id": "818f70e5"
      },
      "outputs": [],
      "source": [
        "#Task 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0f15a0",
      "metadata": {
        "id": "1b0f15a0"
      },
      "source": [
        "We can also plot histograms of the data. Below, smoothed histograms from the seaborn package (imported as sns) let us get an idea of the distribution of percent democratic votes in 2012 (left) and 2016 (right)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57df83a8",
      "metadata": {
        "id": "57df83a8"
      },
      "outputs": [],
      "source": [
        "# Plot histograms:\n",
        "f,ax = plt.subplots(1,2, figsize=(2*3*1.6, 2))\n",
        "for i,col in enumerate(['pct_dem_12','pct_dem_16']):\n",
        "    sns.kdeplot(votes[col].values, shade=True, color='slategrey', ax=ax[i])\n",
        "    ax[i].set_title(col.split('_')[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c314d64",
      "metadata": {
        "id": "7c314d64"
      },
      "outputs": [],
      "source": [
        "# Plot spatial distribution of # dem vote in 2012 and 2016 with histogram.\n",
        "f,ax = plt.subplots(2,2, figsize=(1.6*6 + 1,2.4*3), gridspec_kw=dict(width_ratios=(6,1)))\n",
        "for i,col in enumerate(['pct_dem_12','pct_dem_16']):\n",
        "    votes.plot(col, linewidth=.05, cmap='RdBu', ax=ax[i,0])\n",
        "    ax[i,0].set_title(['2012','2016'][i] + \" % democratic vote\")\n",
        "    ax[i,0].set_xticklabels('')\n",
        "    ax[i,0].set_yticklabels('')\n",
        "    sns.kdeplot(votes[col].values, ax=ax[i,1], vertical=True, shade=True, color='slategrey')\n",
        "    ax[i,1].set_xticklabels('')\n",
        "    ax[i,1].set_ylim(-1,1)\n",
        "f.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d6ec1f",
      "metadata": {
        "id": "e6d6ec1f"
      },
      "source": [
        "### TASK 4: Make a new column on your geopandas dataframe called \"pct_dem_change\" and plot it using the syntax above. Explain the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68ce3ef",
      "metadata": {
        "id": "e68ce3ef"
      },
      "outputs": [],
      "source": [
        "# Task 4: add new column pct_dem_change to votes:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c7a89d",
      "metadata": {
        "id": "c5c7a89d"
      },
      "outputs": [],
      "source": [
        "#Task 4: plot your pct_dem_change variable on a map:\n",
        "f,ax = plt.subplots(1,2, figsize=(1.6*6 + 1,1.6*3), gridspec_kw=dict(width_ratios=(6,1)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946cb5c6",
      "metadata": {
        "id": "946cb5c6"
      },
      "source": [
        "Click on this url to learn more about the variables in this dataset: https://geodacenter.github.io/data-and-lab//county_election_2012_2016-variables/\n",
        "\n",
        "Let's say we want to learn more about what county-level factors influence percent change in democratic vote between (pct_dem_change).\n",
        "\n",
        "There are two types of multicollinearity:\n",
        "\n",
        "* *Intrinsic multicollinearity:* is an artifact of how we make observations. Often our measurements serve as proxies for some latent process (for example, we can measure percent silt, percent sand, and percent clay as proxies for the latent variable of soil texture). There will be slight variability in the information content between each proxy measurement, but they will not be independent of one another.\n",
        "\n",
        "* *Incidental collinearity:* is an artifact of how we sample complex populations. If we collect data from a subsample of the landscape where we don't see all combinations of our predictor variables (do not have good cross replication across our variables). We often induce collinearity in our data just because we are limitted in our ability to sample the environment at the scale of temporal/spatial variability of our process of interest. Incidental collinearity is a model formulation problem.(See here for more info on how to avoid it: https://people.umass.edu/sdestef/NRC%20601/StudyDesignConcepts.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da5b59ac",
      "metadata": {
        "id": "da5b59ac"
      },
      "source": [
        "### TASK 5: Looking at the data description, pick two variables that you believe will be intrinsically multicollinear. List and describe these variables. Why do you think they are intrinsically related?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18207d3e",
      "metadata": {
        "id": "18207d3e"
      },
      "source": [
        "## Multivariate regression in observational data:\n",
        "Our next step is to formulate our predictive/diagnostic model. We want to create a subset of the \"votes\" geopandas data frame that contains ten predictor variables and our response variable (pct_pt_16) two variables you selected under TASK 1. First, create a list of the variables you'd like to select.\n",
        "\n",
        "### TASK 6: Create a subset of votes called \"my_list\" containing only your selected predictor variables. Make sure you use the two variables selected under TASK 5, and eight additional variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d4c6cb",
      "metadata": {
        "id": "65d4c6cb"
      },
      "outputs": [],
      "source": [
        "# Task 4: create a subset of votes called \"my list\" with all your subset variables.\n",
        "#my_list = [\"pct_pt_16\", <list your variables here>]\n",
        "\n",
        "#For example:\n",
        "my_list = [\"pct_dem_change\", #The number in diff_2016 expressed as a percent of the total votes. Negative if fewer votes were cast for the Democratic candidate\n",
        "           \"PST120214\", #Population, percent change - April 1, 2010 to July 1, 2014\n",
        "           \"SEX255214\", #Female persons, percent, 2014\n",
        "           \"RHI125214\", #White alone, percent, 2014\n",
        "           \"RHI225214\", #Black or African American alone, percent, 2014\n",
        "           \"EDU685213\", #Bachelor’s degree or higher, percent of persons age 25+, 2009-2013\n",
        "           \"VET605213\", #Veterans, 2009-2013\n",
        "           \"HSG495213\", #Median value of owner-occupied housing units, 2009-2013\n",
        "           \"HSD310213\", #Persons per household, 2009-2013\n",
        "           \"INC910213\", #Per capita money income in past 12 months (2013 dollars), 2009-2013\n",
        "           \"LND110210\"] #Land area in square miles, 2010\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1835a8d",
      "metadata": {
        "id": "f1835a8d"
      },
      "outputs": [],
      "source": [
        "#check to make sure all your columns are there:\n",
        "votes[my_list].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d28d9dc4",
      "metadata": {
        "id": "d28d9dc4"
      },
      "source": [
        "### Scatterplot matrix\n",
        "We call the process of getting to know your data (ranges and distributions of the data, as well as any relationships between variables) \"exploratory data analysis\". Pairwise plots of your variables, called scatterplots, can provide a lot of insight into the type of relationships you have between variables. A scatterplot matrix is a pairwise comparison of all variables in your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f67ff6",
      "metadata": {
        "id": "f3f67ff6"
      },
      "outputs": [],
      "source": [
        "#Use seaborn.pairplot to plot a scatterplot matrix of you 10 variable subset:\n",
        "sns.pairplot(votes[my_list])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b1b309",
      "metadata": {
        "id": "e7b1b309"
      },
      "source": [
        "### TASK 7: Do you observe any collinearity in this dataset? How would you describe the relationship between your two \"incidentally collinear\" variables that you selected based on looking at variable descriptions?\n",
        "\n",
        "\n",
        "\n",
        "### TASK 8: What is plotted on the diagonal panels of the scatterplot matrix?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b42b91",
      "metadata": {
        "id": "89b42b91"
      },
      "source": [
        "## Diagnosing collinearity globally:\n",
        "Variance Inflation Factor describes the magnitude of variance inflation that can be expected in an OLS parameter estimate for a given variable *given pairwise collinearity between that variable and another variable*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be64d7b",
      "metadata": {
        "id": "9be64d7b"
      },
      "outputs": [],
      "source": [
        "#VIF = 1/(1-R2) of a pairwise OLS regression between two predictor variables\n",
        "#We can use a built-in function \"variance_inflation_factor\" from statsmodel.api to calculate VIF\n",
        "#Learn more about the function\n",
        "?variance_inflation_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53f51dc",
      "metadata": {
        "id": "a53f51dc"
      },
      "outputs": [],
      "source": [
        "#Calculate VIFs on our dataset\n",
        "vif = pd.DataFrame()\n",
        "vif[\"VIF Factor\"] = [variance_inflation_factor(votes[my_list[1:11]].values, i) for i in range(votes[my_list[1:11]].shape[1])]\n",
        "vif[\"features\"] = votes[my_list[1:11]].columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d465f95f",
      "metadata": {
        "id": "d465f95f"
      },
      "outputs": [],
      "source": [
        "vif.round()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae322791",
      "metadata": {
        "id": "ae322791"
      },
      "source": [
        "### Collinearity is always present in observational data. When is it a problem?\n",
        "Generally speaking, including any variables with VIF > 10 is considered \"too much\" collinearity. But this value is somewhat arbitrary. The extent to which variance inflation will impact your analysis is highly context dependent. There are two primary contexts where variance inflation is problematic:\n",
        "\n",
        " 1\\. **You are using your analysis to evaluate variable importance:** If you are using parameter estimates from your model to diagnose which observations have physically important relationships with your response variable, variance inflation can make an important predictor look unimportant, and parameter estimates will be highly leveraged by small changes in the data.\n",
        "\n",
        " 2\\. **You want to use your model to make predictions in a situation where the specific structure of collinearity between variables may have shifted:** When training a model on collinear data, the model only applies to data with that exact structure of collinearity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a7a711",
      "metadata": {
        "id": "f8a7a711"
      },
      "source": [
        "### Caluculate a linear regression on the global data:\n",
        "In this next step, we're going to calculate a linear regression in our data an determine whether there is a statistically significant relationship between per capita income and percent change in democratic vote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "515911da",
      "metadata": {
        "id": "515911da"
      },
      "outputs": [],
      "source": [
        "#first, forumalate the model. See weather_trend.py in \"Git_101\" for a refresher on how.\n",
        "\n",
        "#extract variable that you want to use to \"predict\"\n",
        "X = np.array(votes[my_list[1:11]].values)\n",
        "#standardize data to assist in interpretation of coefficients\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "#extract variable that we want to \"predict\"\n",
        "Y = np.array(votes['pct_dem_change'].values)\n",
        "#standardize data to assist in interpretation of coefficients\n",
        "Y = (Y - np.mean(X)) / np.std(Y)\n",
        "\n",
        "lm = OLS(Y,X)\n",
        "lm_results = OLS(Y,X).fit().summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a265a8",
      "metadata": {
        "id": "68a265a8"
      },
      "outputs": [],
      "source": [
        "print(lm_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d760fd73",
      "metadata": {
        "id": "d760fd73"
      },
      "source": [
        "### TASK 9: Answer: which coefficients indicate a statisticall significant relationship between parameter and pct_dem_change? What is your most important predictor variable? How can you tell?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1202f4f3",
      "metadata": {
        "id": "1202f4f3"
      },
      "source": [
        "### TASK10:  Are any of these parameters subject to variance inflation? How can you tell?\n",
        "\n",
        "\n",
        "**With a cutoff of VIF=10, the following variables are subject to variable inflation.**\n",
        "\n",
        "\t269.0 \tSEX255214\n",
        "\t119.0 \tRHI125214\n",
        "\t20.0 \tEDU685213\n",
        "\t11.0 \tHSG495213\n",
        "\t106.0 \tHSD310213\n",
        "\t67.0 \tINC910213"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25a9235",
      "metadata": {
        "id": "c25a9235"
      },
      "source": [
        "Now, let's plot our residuals to see if there are any spatial patterns in them.\n",
        "\n",
        "Remember residuals = predicted - fitted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0098b6ee",
      "metadata": {
        "id": "0098b6ee"
      },
      "outputs": [],
      "source": [
        "#Add model residuals to our \"votes\" geopandas dataframe:\n",
        "votes['lm_resid']=OLS(Y,X).fit().resid\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vif['OLS_coef']=OLS(Y,X).fit().params"
      ],
      "metadata": {
        "id": "E_TS1YyLQFA8"
      },
      "id": "E_TS1YyLQFA8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a439b312",
      "metadata": {
        "id": "a439b312"
      },
      "outputs": [],
      "source": [
        "sns.kdeplot(votes['lm_resid'].values, shade=True, color='slategrey')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc83f6a",
      "metadata": {
        "id": "0bc83f6a"
      },
      "source": [
        "### TASK 11: Are our residuals normally distributed with a mean of zero? What does that mean?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f59dce",
      "metadata": {
        "id": "66f59dce"
      },
      "source": [
        "## Penalized regression: ridge penalty\n",
        "In penalized regression, we intentionally bias the parameter estimates to stabilize them given collinearity in the dataset.\n",
        "\n",
        "From https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n",
        "\n",
        ">As mentioned before, ridge regression performs ‘L2 regularization‘, i.e. it adds a factor of sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:\n",
        "\n",
        "**Objective = RSS + α * (sum of square of coefficients)**\n",
        "\n",
        "Here, α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. α can take various values:\n",
        "\n",
        "* **α = 0:** The objective becomes same as simple linear regression. We’ll get the same coefficients as simple linear regression.\n",
        "\n",
        "* **α = ∞:** The coefficients will approach zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
        "\n",
        "* **0 < α < ∞:** The magnitude of α will decide the weightage given to different parts of objective. The coefficients will be somewhere between 0 and ones for simple linear regression.\"\n",
        "\n",
        "In other words, the ridge penalty shrinks coefficients such that collinear coefficients will have more similar coefficient values. It has a \"grouping\" tendency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cebcc4a3",
      "metadata": {
        "id": "cebcc4a3"
      },
      "outputs": [],
      "source": [
        "# when L2=0, Ridge equals OLS\n",
        "model = Ridge(alpha=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b4d30d",
      "metadata": {
        "id": "60b4d30d"
      },
      "outputs": [],
      "source": [
        "# define model evaluation method\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "#force scores to be positive\n",
        "scores = absolute(scores)\n",
        "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,Y)\n",
        "#Print out the model coefficients\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "mbYodbd4hfkD"
      },
      "id": "mbYodbd4hfkD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning\n",
        "The L2 coefficient (called alpha above) is a free parameter (hyperparameter) in the model, meaning we can set it whatever value we feel is best.\n",
        "\n",
        "In machine learning, we often try multiple options for these hyperparameters, and select the value with the highest model performance."
      ],
      "metadata": {
        "id": "RUmJfpeJhhFE"
      },
      "id": "RUmJfpeJhhFE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on a range of alphas\n",
        "alpha = np.arange(0,20,0.1)\n",
        "ridge_tune=pd.DataFrame(alpha, columns=[\"alpha\"])\n",
        "ridge_tune['Score']=0\n",
        "i=0\n",
        "for a in alpha:\n",
        "  model= Ridge(alpha=a)\n",
        "  oos_scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "  #force scores to be positive\n",
        "  oos_scores = absolute(oos_scores)\n",
        "  ridge_tune.Score.loc[i]=mean(oos_scores)\n",
        "  i=i+1\n"
      ],
      "metadata": {
        "id": "LYlRKrRrC_eG"
      },
      "id": "LYlRKrRrC_eG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Plot model performance as a function of alpha\n",
        "plt.plot(ridge_tune.alpha, ridge_tune.Score)\n",
        "plt.show()\n",
        "# What is the y-axis label? What is the x-axis label?"
      ],
      "metadata": {
        "id": "fWJPA8ifIHSb"
      },
      "id": "fWJPA8ifIHSb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the value of alpha which minimizes MAE\n",
        "a2 = ridge_tune.alpha.iloc[ridge_tune['Score'].idxmin()]\n",
        "a2"
      ],
      "metadata": {
        "id": "dOdAzM_YJxHx"
      },
      "id": "dOdAzM_YJxHx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c68402c6",
      "metadata": {
        "id": "c68402c6"
      },
      "outputs": [],
      "source": [
        "# Train model with optimized alpha\n",
        "model = Ridge(alpha=a2)\n",
        "model.fit(X,Y)\n",
        "#Print out the model coefficients\n",
        "print(model.coef_)\n",
        "vif['Ridge_coef']=model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d94d1f",
      "metadata": {
        "id": "53d94d1f"
      },
      "source": [
        "## Penalized regression: lasso penalty\n",
        "\n",
        "From https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n",
        "> LASSO stands for Least Absolute Shrinkage and Selection Operator.\n",
        "\n",
        "There are 2 key words here – **absolute** and **selection**.\n",
        "\n",
        "Lets consider the former first and worry about the latter later.\n",
        ".\n",
        "Lasso regression performs L1 regularization, i.e. it adds a factor to the sum of absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:\n",
        "\n",
        "**Objective = RSS + α * (sum of absolute value of coefficients)**\n",
        "\n",
        "Here, α (alpha) works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients. Like that of ridge, α can take various values. Lets iterate it here briefly:\n",
        "\n",
        "* **α = 0:** Same coefficients as simple linear regression\n",
        "* **α = ∞:** All coefficients zero (same logic as before)\n",
        "* **0 < α < ∞:** coefficients between 0 and that of simple linear regression\n",
        "\n",
        "The lasso penalty shrinks unimportant coefficients down towards zero, automatically \"selecting\" important predictor variables. But what if that shrunken coefficient is induced by incidental collinearity (i.e. is a feature of how we sampled our data)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f683d0",
      "metadata": {
        "id": "17f683d0"
      },
      "outputs": [],
      "source": [
        "# when L1=0, Lasso equals OLS\n",
        "model = Lasso(alpha=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eccdbab",
      "metadata": {
        "id": "5eccdbab"
      },
      "outputs": [],
      "source": [
        "# define model evaluation method\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "#force scores to be positive\n",
        "scores = absolute(scores)\n",
        "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,Y)\n",
        "#Print out the model coefficients\n",
        "print(model.coef_)\n",
        "#How do these compare to OLS coefficients?"
      ],
      "metadata": {
        "id": "WtYefDGhh29N"
      },
      "id": "WtYefDGhh29N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when L1 approaches infinity, coefficients will become exactly zero, and MAE equals the variance of our response variable:\n",
        "model = Lasso(alpha=10000000)"
      ],
      "metadata": {
        "id": "PGLGB4Fmh9He"
      },
      "id": "PGLGB4Fmh9He",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model evaluation method\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "#force scores to be positive\n",
        "scores = absolute(scores)\n",
        "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "metadata": {
        "id": "pX1r-KF6iEkD"
      },
      "id": "pX1r-KF6iEkD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6fd8e0a",
      "metadata": {
        "id": "d6fd8e0a"
      },
      "outputs": [],
      "source": [
        "model.fit(X,Y)\n",
        "#Print out the model coefficients\n",
        "print(model.coef_)\n",
        "#How do these compare with OLS coefficients above?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning\n",
        "Just like with the ridge penalty, we want to select the value of alpha that minimizes error in the lasso model.\n",
        "\n",
        "We'll use hyperparameter tuning again to do this:"
      ],
      "metadata": {
        "id": "Q8od0fBRiNgc"
      },
      "id": "Q8od0fBRiNgc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define range of alphas:\n",
        "alpha = np.arange(0,.02,0.0001)\n",
        "lasso_tune=pd.DataFrame(alpha, columns=[\"alpha\"])\n",
        "lasso_tune['Score']=0\n",
        "i=0\n",
        "for a in alpha:\n",
        "  model= Lasso(alpha=a)\n",
        "  oos_scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "  #force scores to be positive\n",
        "  oos_scores = absolute(oos_scores)\n",
        "  lasso_tune.Score.loc[i]=mean(oos_scores)\n",
        "  i=i+1\n"
      ],
      "metadata": {
        "id": "7HoHTeetPoP5"
      },
      "id": "7HoHTeetPoP5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot model performance as a function of alpha:\n",
        "plt.plot(lasso_tune.alpha, lasso_tune.Score)\n",
        "plt.show()\n",
        "# What is the y-axis label? What is the x-axis label?"
      ],
      "metadata": {
        "id": "v3Fr81sEPysW"
      },
      "id": "v3Fr81sEPysW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = lasso_tune.alpha.iloc[lasso_tune['Score'].idxmin()]\n",
        "a1"
      ],
      "metadata": {
        "id": "WyqaEXZsR6uX"
      },
      "id": "WyqaEXZsR6uX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model with optimal alpha1\n",
        "model= Lasso(alpha=a1)\n",
        "model.fit(X,Y)\n",
        "#Print out the model coefficients\n",
        "print(model.coef_)\n",
        "\n",
        "vif['LASSO_coef']= model.coef_"
      ],
      "metadata": {
        "id": "5gm-aAixSNI7"
      },
      "id": "5gm-aAixSNI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5fe1b7ff",
      "metadata": {
        "id": "5fe1b7ff"
      },
      "source": [
        "### Penalized regression: elastic net penalty\n",
        "\n",
        "In other words, the lasso penalty shrinks unimportant coefficients down towards zero, automatically \"selecting\" important predictor variables. The ridge penalty shrinks coefficients of collinear predictor variables nearer to each other, effectively partitioning the magnitude of response from the response variable between them, instead of \"arbitrarily\" partitioning it to one group.\n",
        "\n",
        "We can also run a regression with a linear combination of ridge and lasso, called the elastic net, that has a cool property called \"group selection.\"\n",
        "\n",
        "The ridge penalty still works to distribute response variance equally between members of \"groups\" of collinear predictor variables. The lasso penalty still works to shrink certain coefficients to exactly zero so they can be ignored in model formulation. The elastic net produces models that are both sparse and stable under collinearity, by shrinking parameters of members of unimportant collinear predictor variables to exactly zero:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the parameters in the ElasticNet fuction.\n",
        "# Which variable represents the ridge coefficient? Which parameter represents the lasso coefficient?\n",
        "?ElasticNet"
      ],
      "metadata": {
        "id": "RgMVSI0Ri6vR"
      },
      "id": "RgMVSI0Ri6vR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6967ae",
      "metadata": {
        "id": "ce6967ae"
      },
      "outputs": [],
      "source": [
        "# when L1 approaches infinity, certain coefficients will become exactly zero, and MAE equals the variance of our response variable:\n",
        "model = ElasticNet(alpha=.1, l1_ratio=.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fc92a2",
      "metadata": {
        "id": "22fc92a2"
      },
      "outputs": [],
      "source": [
        "# define model evaluation method\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "#force scores to be positive\n",
        "scores = absolute(scores)\n",
        "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fee71d5",
      "metadata": {
        "id": "0fee71d5"
      },
      "outputs": [],
      "source": [
        "model.fit(X,Y)\n",
        "#Print out the model coefficients\n",
        "print(model.coef_)\n",
        "#How do these compare with OLS coefficients above?\n",
        "vif[\"ElNet_coef\"]=model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac8a540d",
      "metadata": {
        "id": "ac8a540d"
      },
      "source": [
        "### TASK 12: Match these elastic net coefficients up with your original data. Do you see a logical grouping(s) between variables that have non-zero coefficients?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c12c783",
      "metadata": {
        "id": "6c12c783"
      },
      "outputs": [],
      "source": [
        "print(vif)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Credit:\n",
        "We saw how to tune individual parameters above. The elastic net regression here has two free parameters. How can you select the best values for these two free parameters?\n",
        "\n",
        "Demonstrate in the cell below."
      ],
      "metadata": {
        "id": "iLhCXRVcjEc8"
      },
      "id": "iLhCXRVcjEc8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra credit scratch cell:"
      ],
      "metadata": {
        "id": "Tjlf88d2jH2C"
      },
      "id": "Tjlf88d2jH2C",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "geostats_env",
      "language": "python",
      "name": "geostats_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}